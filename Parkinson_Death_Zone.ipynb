{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1DWkzQnuZwfH9f9h1rT3U0qbMYqW8YEsz",
      "authorship_tag": "ABX9TyN1YoMC+vYVfi8mE+P9ILRK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatihonay/Deep-Learning-Journey/blob/main/Parkinson_Death_Zone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Libraries"
      ],
      "metadata": {
        "id": "pR3MjL_9LVrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mne torch torchvision braindecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-C5caXgX6kO",
        "outputId": "4e9a9a43-3c1c-4ff0-df77-b6f7f3bdc732"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: torchaudio>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.9.0+cu126)\n",
            "Requirement already satisfied: mne_bids>=0.18 in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.18.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.15.1)\n",
            "Requirement already satisfied: skorch>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.5.3)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (from braindecode) (4.3.0)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.19.1)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.0.0)\n",
            "Requirement already satisfied: rotary_embedding_torch in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.8.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.3.0->braindecode) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.3.0->braindecode) (0.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.3.12)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.8.1)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (3.13.3)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (2.3.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.22.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb->braindecode) (2025.3)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode) (0.11.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2026.1.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.3.0->braindecode) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb->braindecode) (2.0.0)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->linear_attention_transformer->braindecode) (0.4.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Scan All Subject and Detect Common EEG Channels in Dataset\n"
      ],
      "metadata": {
        "id": "GvZbBR3_Lm3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3t0Gz3RX2T2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import mne\n",
        "\n",
        "dataset_root = '/content/drive/MyDrive/ds007020'\n",
        "\n",
        "all_channel_sets = []\n",
        "failed_subjects_channel_check = []\n",
        "\n",
        "# Get a list of all subject directories that start with 'sub-'\n",
        "subject_dirs = [d for d in os.listdir(dataset_root) if d.startswith('sub-') and os.path.isdir(os.path.join(dataset_root, d))]\n",
        "\n",
        "print(f\"Found {len(subject_dirs)} potential subject directories for channel check.\")\n",
        "\n",
        "for subject_id in sorted(subject_dirs): # Sort for consistent processing order\n",
        "    subject_path = os.path.join(dataset_root, subject_id)\n",
        "\n",
        "    # Look for session directories (e.g., 'ses-01')\n",
        "    session_dirs = [d for d in os.listdir(subject_path) if d.startswith('ses-') and os.path.isdir(os.path.join(subject_path, d))]\n",
        "\n",
        "    if not session_dirs:\n",
        "        # print(f\"Warning: No session directories found for {subject_id}. Skipping channel check.\")\n",
        "        failed_subjects_channel_check.append(subject_id)\n",
        "        continue\n",
        "\n",
        "    # Assuming 'ses-01' is the primary session or taking the first available session\n",
        "    target_session_id = 'ses-01'\n",
        "    if target_session_id in session_dirs:\n",
        "        session_id = target_session_id\n",
        "    elif session_dirs:\n",
        "        session_id = session_dirs[0] # Use the first found session if 'ses-01' is not present\n",
        "        # print(f\"Warning: '{target_session_id}' not found for {subject_id}, using '{session_id}' instead for channel check.\")\n",
        "    else:\n",
        "        # This case is already covered by the 'if not session_dirs' above, but kept for clarity\n",
        "        # print(f\"Warning: No valid session found for {subject_id}. Skipping channel check.\")\n",
        "        failed_subjects_channel_check.append(subject_id)\n",
        "        continue\n",
        "\n",
        "    session_path = os.path.join(subject_path, session_id)\n",
        "    eeg_folder_path = os.path.join(session_path, 'eeg')\n",
        "\n",
        "    if not os.path.isdir(eeg_folder_path):\n",
        "        # print(f\"Warning: 'eeg' directory not found for {subject_id}/{session_id}. Skipping channel check.\")\n",
        "        failed_subjects_channel_check.append(subject_id)\n",
        "        continue\n",
        "\n",
        "    # Find the .vhdr file within the 'eeg' directory\n",
        "    vhdr_files = [f for f in os.listdir(eeg_folder_path) if f.endswith('.vhdr')]\n",
        "\n",
        "    if not vhdr_files:\n",
        "        # print(f\"Warning: No .vhdr file found for {subject_id}/{session_id}. Skipping channel check.\")\n",
        "        failed_subjects_channel_check.append(subject_id)\n",
        "        continue\n",
        "\n",
        "    # Assuming there's only one .vhdr file per eeg directory, or taking the first one\n",
        "    vhdr_file_name = vhdr_files[0]\n",
        "    full_vhdr_path = os.path.join(eeg_folder_path, vhdr_file_name)\n",
        "\n",
        "    try:\n",
        "        # Load the raw EEG data (preload=False to minimize memory usage during this scan)\n",
        "        raw = mne.io.read_raw_brainvision(full_vhdr_path, preload=False, verbose=False)\n",
        "        all_channel_sets.append(set(raw.info['ch_names']))\n",
        "        # print(f\"Processed {subject_id}/{session_id}: Found {len(raw.info['ch_names'])} channels.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {subject_id}/{session_id} ({full_vhdr_path}) for channels: {e}\")\n",
        "        failed_subjects_channel_check.append(subject_id)\n",
        "\n",
        "print(f\"\\nSuccessfully collected channel information from {len(all_channel_sets)} subjects.\")\n",
        "print(f\"Failed to collect channel information from {len(failed_subjects_channel_check)} subjects: {failed_subjects_channel_check}\")\n",
        "\n",
        "if all_channel_sets:\n",
        "    # Find the intersection of all channel sets to get common channels\n",
        "    common_channels = set.intersection(*all_channel_sets)\n",
        "    print(f\"\\nCommon channels across all successfully processed subjects ({len(common_channels)} channels):\")\n",
        "    print(sorted(list(common_channels)))\n",
        "else:\n",
        "    print(\"No channel sets were collected. Cannot determine common channels.\")\n",
        "\n",
        "\n",
        "# Save common channels to a text file for later use\n",
        "common_channels_list = sorted(list(common_channels))\n",
        "output_file = os.path.join(dataset_root, 'common_channels.txt')\n",
        "with open(output_file, 'w') as f:\n",
        "    for ch in common_channels_list:\n",
        "        f.write(ch + '\\n')\n",
        "print(f\"Common channels saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Check All Subjects with the Common EEG Channels"
      ],
      "metadata": {
        "id": "WvADcR5eL7HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_subjects_set = set(failed_subjects_channel_check)\n",
        "valid_subject_dirs = [s for s in subject_dirs if s not in failed_subjects_set]\n",
        "\n",
        "print(f\"Original number of subject directories: {len(subject_dirs)}\")\n",
        "print(f\"Number of subjects that failed channel check: {len(failed_subjects_channel_check)}\")\n",
        "print(f\"Number of valid subject directories after filtering: {len(valid_subject_dirs)}\")\n",
        "print(f\"First 5 valid subject directories: {valid_subject_dirs[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTVpEFXTY1e5",
        "outputId": "64ed8397-ca41-4171-df2a-2e8aeccb609f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of subject directories: 94\n",
            "Number of subjects that failed channel check: 0\n",
            "Number of valid subject directories after filtering: 94\n",
            "First 5 valid subject directories: ['sub-PD1541', 'sub-PD1341', 'sub-PD1281', 'sub-PD1051', 'sub-PD1591']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Load and Clean Data\n",
        "\n",
        "ASR for artifact removal process, and then perform epoching on the resting dataset."
      ],
      "metadata": {
        "id": "GX9yTLPhMxzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "from mne import make_fixed_length_epochs\n",
        "from asrpy import ASR  # Import ASR\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "SFREQ_TARGET = 100\n",
        "L_FREQ, H_FREQ = 0.5, 45.0\n",
        "WINDOW_SIZE_SEC = 5\n",
        "OVERLAP = 5.0  # Changed to 5.0 seconds (50% overlap) for better data augmentation\n",
        "ASR_CUTOFF = 20 # Standard cutoff for artifact removal (lower = more aggressive)\n",
        "\n",
        "# --- 1. LOAD LABELS ---\n",
        "participants_path = os.path.join(dataset_root, 'participants.tsv')\n",
        "df_participants = pd.read_csv(participants_path, sep='\\t')\n",
        "\n",
        "label_map = {}\n",
        "for idx, row in df_participants.iterrows():\n",
        "    sub_id = row['participant_id']\n",
        "    status = row['survival_status']\n",
        "    if status == 'living':\n",
        "        label_map[sub_id] = 0\n",
        "    elif status == 'deceased':\n",
        "        label_map[sub_id] = 1\n",
        "\n",
        "print(f\"Loaded labels for {len(label_map)} subjects.\")\n",
        "\n",
        "# --- 2. DATA PROCESSING LOOP ---\n",
        "X_list = []\n",
        "y_list = []\n",
        "groups_list = []\n",
        "\n",
        "# Ensure channels are consistent\n",
        "target_channels = sorted(list(common_channels))\n",
        "print(f\"Starting processing using {len(target_channels)} channels...\")\n",
        "\n",
        "for subject_id in sorted(subject_dirs):\n",
        "    if subject_id not in label_map:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Construct path\n",
        "        session_dirs = [d for d in os.listdir(os.path.join(dataset_root, subject_id)) if d.startswith('ses-')]\n",
        "        if not session_dirs: continue\n",
        "        session_id = 'ses-01' if 'ses-01' in session_dirs else session_dirs[0]\n",
        "\n",
        "        eeg_folder = os.path.join(dataset_root, subject_id, session_id, 'eeg')\n",
        "        vhdr_files = [f for f in os.listdir(eeg_folder) if f.endswith('.vhdr')]\n",
        "        if not vhdr_files: continue\n",
        "        full_path = os.path.join(eeg_folder, vhdr_files[0])\n",
        "\n",
        "        # A. LOAD\n",
        "        raw = mne.io.read_raw_brainvision(full_path, preload=True, verbose='ERROR')\n",
        "\n",
        "        # B. PICK CHANNELS\n",
        "        raw.pick_channels(target_channels)\n",
        "\n",
        "        # C. FILTER & RESAMPLE\n",
        "        raw.filter(L_FREQ, H_FREQ, verbose='ERROR')\n",
        "        raw.resample(SFREQ_TARGET, verbose='ERROR')\n",
        "\n",
        "        # --- NEW: ASR CLEANING (Inserted Here) ---\n",
        "        # ASR needs the continuous raw data to learn what \"clean\" looks like.\n",
        "        try:\n",
        "            asr = ASR(sfreq=SFREQ_TARGET, cutoff=ASR_CUTOFF)\n",
        "            asr.fit(raw)            # Learn clean statistics from this subject\n",
        "            raw = asr.transform(raw) # Repair artifacts\n",
        "        except Exception as asr_e:\n",
        "            print(f\"  ASR failed for {subject_id}, using standard data: {asr_e}\")\n",
        "        # -----------------------------------------\n",
        "\n",
        "        # D. CREATE WINDOWS (EPOCHS)\n",
        "        epochs = make_fixed_length_epochs(\n",
        "            raw,\n",
        "            duration=WINDOW_SIZE_SEC,\n",
        "            overlap=OVERLAP,\n",
        "            preload=True,\n",
        "            verbose='ERROR'\n",
        "        )\n",
        "\n",
        "        # E. EXTRACT DATA\n",
        "        data = epochs.get_data(copy=True)\n",
        "\n",
        "        # F. APPEND\n",
        "        label = label_map[subject_id]\n",
        "        n_windows = data.shape[0]\n",
        "\n",
        "        if n_windows > 0:\n",
        "            X_list.append(data)\n",
        "            y_list.extend([label] * n_windows)\n",
        "            groups_list.extend([subject_id] * n_windows)\n",
        "            print(f\"Processed {subject_id}: {n_windows} windows (Label: {label})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {subject_id}: {e}\")\n",
        "\n",
        "# --- 3. CONVERT TO NUMPY ---\n",
        "if len(X_list) > 0:\n",
        "    X = np.concatenate(X_list, axis=0)\n",
        "    y = np.array(y_list)\n",
        "    groups = np.array(groups_list)\n",
        "\n",
        "    # --- 4. SCALING ---\n",
        "    mean = np.mean(X, axis=(0, 2), keepdims=True)\n",
        "    std = np.std(X, axis=(0, 2), keepdims=True)\n",
        "    std[std == 0] = 1.0\n",
        "    X_scaled = (X - mean) / (std + 1e-6)\n",
        "\n",
        "    print(\"\\n--- DATA LOADING COMPLETE ---\")\n",
        "    print(f\"Final Data Shape (X): {X.shape}\")\n",
        "    print(f\"Final Labels Shape (y): {y.shape}\")\n",
        "    print(f\"Class Balance: {np.sum(y==0)} Living vs {np.sum(y==1)} Deceased\")\n",
        "else:\n",
        "    print(\"Error: No data was processed.\")"
      ],
      "metadata": {
        "id": "WtQjvuy-mKuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Save pre-processed Data for Later Use"
      ],
      "metadata": {
        "id": "LoMLBgEoNlnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. SAVE DATA ---\n",
        "import pickle  # Add this import at the top if not already present\n",
        "\n",
        "output_dir = os.path.join(dataset_root, 'processed_data')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save with pickle (recommended for large numpy arrays)\n",
        "data_dict = {\n",
        "    'X': X,\n",
        "    'X_scaled': X_scaled,\n",
        "    'y': y,\n",
        "    'groups': groups,\n",
        "    'mean': mean,\n",
        "    'std': std,\n",
        "    'target_channels': target_channels,\n",
        "    'config': {\n",
        "        'SFREQ_TARGET': SFREQ_TARGET,\n",
        "        'L_FREQ': L_FREQ,\n",
        "        'H_FREQ': H_FREQ,\n",
        "        'WINDOW_SIZE_SEC': WINDOW_SIZE_SEC,\n",
        "        'OVERLAP': OVERLAP\n",
        "    }\n",
        "}\n",
        "\n",
        "pickle_file = os.path.join(output_dir, 'eeg_data.pkl')\n",
        "with open(pickle_file, 'wb') as f:\n",
        "    pickle.dump(data_dict, f)\n",
        "\n",
        "print(f\"Data saved to: {pickle_file}\")\n",
        "print(f\"Filesize: {os.path.getsize(pickle_file) / (1024**2):.1f} MB\")\n"
      ],
      "metadata": {
        "id": "4mftAI2QKppL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Import Channel List and Cleaned EEG Data\n"
      ],
      "metadata": {
        "id": "a2FtC0swLEaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Channels List\n",
        "\n",
        "import os\n",
        "\n",
        "input_file = os.path.join(dataset_root, 'common_channels.txt')\n",
        "\n",
        "with open(input_file, 'r') as f:\n",
        "    common_channels = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(common_channels)\n",
        "print(type(common_channels))  # list\n",
        "\n",
        "# Import Data\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# --- LOAD PREVIOUSLY PROCESSED DATA ---\n",
        "dataset_root = '/content/drive/MyDrive/ds007020'\n",
        "processed_dir = os.path.join(dataset_root, 'processed_data')\n",
        "pickle_file = os.path.join(processed_dir, 'eeg_data.pkl')\n",
        "\n",
        "print(\"Loading processed EEG data...\")\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "# Extract all your variables\n",
        "X = data_dict['X']\n",
        "X_scaled = data_dict['X_scaled']\n",
        "y = data_dict['y']\n",
        "groups = data_dict['groups']\n",
        "mean = data_dict['mean']\n",
        "std = data_dict['std']\n",
        "target_channels = data_dict['target_channels']\n",
        "config = data_dict['config']\n",
        "\n",
        "# Print loaded info\n",
        "print(f\"Loaded X shape: {X.shape}\")\n",
        "print(f\"Loaded X_scaled shape: {X_scaled.shape}\")\n",
        "print(f\"Labels: {np.sum(y==0)} Living vs {np.sum(y==1)} Deceased\")\n",
        "print(f\"Channels: {len(target_channels)}\")\n",
        "print(f\"SFREQ: {config['SFREQ_TARGET']} Hz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSRcQ41Ia9yR",
        "outputId": "fb047ae0-d27d-43e2-a974-fdddbd5ff746"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scaled to mean 0, std 1.\n"
          ]
        }
      ]
    }
  ]
}